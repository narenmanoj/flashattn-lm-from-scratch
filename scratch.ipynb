{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from GPTDatasetV1 import GPTDatasetV1\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from custom_modules import (\n",
    "    TransformerLM,\n",
    "    AdamW,\n",
    "    cross_entropy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    " \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    " \"of someunknownPlace.\"\n",
    ")\n",
    "tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "strings = tokenizer.decode(tokens)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"d_model\": 512,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_heads\": 16,\n",
    "    \"d_ff\": 1344,\n",
    "    \"rope_theta\": 1e4,\n",
    "    \"context_length\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45372d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "\n",
    "dataloader = GPTDatasetV1.create_dataloader(raw_text,\n",
    "                                            batch_size=8,\n",
    "                                            shuffle=False,\n",
    "                                            stride=1,\n",
    "                                            max_length=hyperparams[\"context_length\"],\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLM(vocab_size=vocab_size, \n",
    "                      context_length=hyperparams[\"context_length\"],\n",
    "                      d_model=hyperparams[\"d_model\"],\n",
    "                      num_layers=hyperparams[\"num_layers\"],\n",
    "                      num_heads=hyperparams[\"num_heads\"],\n",
    "                      d_ff=hyperparams[\"d_ff\"],\n",
    "                      rope_theta=hyperparams[\"rope_theta\"],\n",
    "                      device=device)\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss_fn = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460dd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, loss_fn, optimizer, model):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "tb_writer = SummaryWriter('runs/the_verdict_{}'.format(timestamp))\n",
    "\n",
    "print(next(model.parameters()).device)\n",
    "\n",
    "train_one_epoch(epoch_index=0, tb_writer=tb_writer, loss_fn=loss_fn, optimizer=optimizer, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
